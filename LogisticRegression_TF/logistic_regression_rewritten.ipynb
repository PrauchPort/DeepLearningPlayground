{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitdeeplearningcondaa9daeba634b04f9786f392bcf0f53381",
   "display_name": "Python 3.6.10 64-bit ('deeplearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://builtin.com/data-science/guide-logistic-regression-tensorflow-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "train_x, test_x = np.array(train_x, np.float32), np.array(test_x, np.float32)\n",
    "\n",
    "train_x, test_x = train_x.reshape([train_x.shape[0], -1]), test_x.reshape([test_x.shape[0], -1])\n",
    "\n",
    "train_x, test_x = train_x / 255., test_x / 255.\n",
    "\n",
    "# train_y, test_y = train_y.reshape([train_y.shape[0], -1]), test_y.reshape([-1, test_y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch_size = 256\n",
    "num_features = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.ones([num_features, num_classes]), tf.float32)\n",
    "\n",
    "b = tf.Variable(tf.zeros([num_classes]), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x):\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "\n",
    "    # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x, y):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        y_pred = logistic_regression(x)\n",
    "\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "\n",
    "    gradients = tape.gradient(loss, [W, b])\n",
    "\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "step: 10, loss: 534.825500, accuracy: 0.878906\nstep: 20, loss: 751.837036, accuracy: 0.832031\nstep: 30, loss: 574.558289, accuracy: 0.855469\nstep: 40, loss: 450.641968, accuracy: 0.871094\nstep: 50, loss: 397.060028, accuracy: 0.890625\nstep: 60, loss: 652.813354, accuracy: 0.832031\nstep: 70, loss: 457.269806, accuracy: 0.863281\nstep: 80, loss: 449.810577, accuracy: 0.878906\nstep: 90, loss: 497.106018, accuracy: 0.886719\nstep: 100, loss: 500.773285, accuracy: 0.843750\nstep: 110, loss: 670.496582, accuracy: 0.835938\nstep: 120, loss: 595.858337, accuracy: 0.839844\nstep: 130, loss: 423.309845, accuracy: 0.886719\nstep: 140, loss: 653.525146, accuracy: 0.816406\nstep: 150, loss: 651.260986, accuracy: 0.843750\nstep: 160, loss: 699.299683, accuracy: 0.757812\nstep: 170, loss: 827.714539, accuracy: 0.800781\nstep: 180, loss: 578.608887, accuracy: 0.855469\nstep: 190, loss: 462.971802, accuracy: 0.855469\nstep: 200, loss: 665.651794, accuracy: 0.843750\nstep: 210, loss: 548.395020, accuracy: 0.863281\nstep: 220, loss: 626.729004, accuracy: 0.867188\nstep: 230, loss: 515.375183, accuracy: 0.875000\nstep: 240, loss: 489.370239, accuracy: 0.894531\nstep: 250, loss: 540.637207, accuracy: 0.882812\nstep: 260, loss: 712.310425, accuracy: 0.816406\nstep: 270, loss: 550.403564, accuracy: 0.855469\nstep: 280, loss: 403.511047, accuracy: 0.902344\nstep: 290, loss: 563.141418, accuracy: 0.839844\nstep: 300, loss: 544.006592, accuracy: 0.832031\nstep: 310, loss: 519.030396, accuracy: 0.855469\nstep: 320, loss: 804.877502, accuracy: 0.796875\nstep: 330, loss: 524.809204, accuracy: 0.863281\nstep: 340, loss: 570.101990, accuracy: 0.859375\nstep: 350, loss: 493.695129, accuracy: 0.863281\nstep: 360, loss: 526.621765, accuracy: 0.851562\nstep: 370, loss: 491.713013, accuracy: 0.847656\nstep: 380, loss: 602.075073, accuracy: 0.843750\nstep: 390, loss: 545.135254, accuracy: 0.847656\nstep: 400, loss: 676.580078, accuracy: 0.785156\nstep: 410, loss: 412.188629, accuracy: 0.863281\nstep: 420, loss: 568.391174, accuracy: 0.855469\nstep: 430, loss: 623.859070, accuracy: 0.839844\nstep: 440, loss: 619.150330, accuracy: 0.835938\nstep: 450, loss: 482.091125, accuracy: 0.902344\nstep: 460, loss: 575.247192, accuracy: 0.867188\nstep: 470, loss: 666.146667, accuracy: 0.855469\nstep: 480, loss: 544.584961, accuracy: 0.867188\nstep: 490, loss: 610.691711, accuracy: 0.847656\nstep: 500, loss: 668.299866, accuracy: 0.828125\nstep: 510, loss: 562.600281, accuracy: 0.839844\nstep: 520, loss: 578.272339, accuracy: 0.855469\nstep: 530, loss: 524.425171, accuracy: 0.871094\nstep: 540, loss: 493.335999, accuracy: 0.882812\nstep: 550, loss: 419.925415, accuracy: 0.894531\nstep: 560, loss: 475.218567, accuracy: 0.871094\nstep: 570, loss: 522.010986, accuracy: 0.859375\nstep: 580, loss: 625.827026, accuracy: 0.851562\nstep: 590, loss: 515.941528, accuracy: 0.875000\nstep: 600, loss: 583.618469, accuracy: 0.855469\nstep: 610, loss: 494.206848, accuracy: 0.882812\nstep: 620, loss: 581.343079, accuracy: 0.832031\nstep: 630, loss: 526.366699, accuracy: 0.859375\nstep: 640, loss: 599.419800, accuracy: 0.847656\nstep: 650, loss: 626.379150, accuracy: 0.820312\nstep: 660, loss: 775.531494, accuracy: 0.812500\nstep: 670, loss: 700.150024, accuracy: 0.839844\nstep: 680, loss: 720.526428, accuracy: 0.843750\nstep: 690, loss: 541.352539, accuracy: 0.871094\nstep: 700, loss: 522.377869, accuracy: 0.875000\nstep: 710, loss: 417.346802, accuracy: 0.902344\nstep: 720, loss: 547.788086, accuracy: 0.871094\nstep: 730, loss: 620.515930, accuracy: 0.851562\nstep: 740, loss: 529.043030, accuracy: 0.875000\nstep: 750, loss: 624.150818, accuracy: 0.804688\nstep: 760, loss: 623.749146, accuracy: 0.867188\nstep: 770, loss: 560.785706, accuracy: 0.859375\nstep: 780, loss: 536.145813, accuracy: 0.863281\nstep: 790, loss: 645.823242, accuracy: 0.867188\nstep: 800, loss: 401.714172, accuracy: 0.894531\nstep: 810, loss: 313.555084, accuracy: 0.882812\nstep: 820, loss: 496.985229, accuracy: 0.890625\nstep: 830, loss: 751.053589, accuracy: 0.789062\nstep: 840, loss: 562.809448, accuracy: 0.867188\nstep: 850, loss: 518.000427, accuracy: 0.878906\nstep: 860, loss: 622.619385, accuracy: 0.824219\nstep: 870, loss: 615.211914, accuracy: 0.855469\nstep: 880, loss: 753.178101, accuracy: 0.820312\nstep: 890, loss: 422.588135, accuracy: 0.863281\nstep: 900, loss: 632.382751, accuracy: 0.859375\nstep: 910, loss: 653.943665, accuracy: 0.851562\nstep: 920, loss: 448.764099, accuracy: 0.851562\nstep: 930, loss: 655.357361, accuracy: 0.835938\nstep: 940, loss: 497.094879, accuracy: 0.855469\nstep: 950, loss: 559.768982, accuracy: 0.871094\nstep: 960, loss: 486.740845, accuracy: 0.882812\nstep: 970, loss: 533.047180, accuracy: 0.859375\nstep: 980, loss: 493.543549, accuracy: 0.878906\nstep: 990, loss: 679.849915, accuracy: 0.832031\nstep: 1000, loss: 691.851990, accuracy: 0.835938\n"
    }
   ],
   "source": [
    "for step, (batch_x, batch_y) in enumerate(train_data.take(1000), 1):\n",
    "\n",
    "    run_optimization(batch_x, batch_y)\n",
    "\n",
    "\n",
    "    if step % 10 == 0:\n",
    "\n",
    "        pred = logistic_regression(batch_x)\n",
    "\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "\n",
    "        acc = accuracy(pred, batch_y)\n",
    "\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}